{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbu018GdTwHPCamZ19BKdA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singhabhisheknitp/Pytorchsimplified/blob/main/Pytorchification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MOuK4Mnwubcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "kIadW7rT2cv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS  torch.nn.ReLU"
      ],
      "metadata": {
        "id": "RrBmjUNfL2yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ReLU:\n",
        "\n",
        "    # y = max(0, x)\n",
        "    def __call__(self, x):\n",
        "        return torch.max(torch.zeros_like(x), x)\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Tanh:\n",
        "\n",
        "     # y = tanh(x)\n",
        "    def __call__(self, x):\n",
        "        return torch.tanh(x)\n",
        "\n",
        "    def parameters(self):\n",
        "        return []"
      ],
      "metadata": {
        "id": "Eq3yCxwG4JxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS    torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, _freeze=False, device=None, dtype=None)"
      ],
      "metadata": {
        "id": "m43Zae2ButmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding:\n",
        "\n",
        "  #weight initialization\n",
        "  def __init__(self, num_embeddings, embedding_dim):\n",
        "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
        "\n",
        "  # Y = C[X]   where C is embedding matrix or lookup table of which we pluck rows as per index given by X datasets\n",
        "  def __call__(self, IX):\n",
        "    self.out = self.weight[IX]\n",
        "    return self.out\n",
        "\n",
        "  # keeping learnable parameters in parameter fucntion to access for setting grad true & updating the gradients\n",
        "  def parameters(self):\n",
        "    return [self.weight]"
      ],
      "metadata": {
        "id": "Vf_AWzF3uuH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS  torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)"
      ],
      "metadata": {
        "id": "st1my2j4teCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7epGsApHs5wu"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "\n",
        "  #weight initialization\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "  # defining forward pass  Y = x@W + b\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  # keeping learnable parameters in parameter fucntion to access for setting grad true & updating the gradients\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS   torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)"
      ],
      "metadata": {
        "id": "ttAZ1XPjuKB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      if x.ndim == 2:\n",
        "        dim = 0\n",
        "      elif x.ndim == 3:\n",
        "        dim = (0,1)\n",
        "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
        "      xvar = x.var(dim, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "Eu7IE0bjulX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS    torch.nn.Flatten(start_dim=1, end_dim=- 1)"
      ],
      "metadata": {
        "id": "URPV1RmMu_Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlattenConsecutive:\n",
        "\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    x = x.view(B, T//self.n, C*self.n)\n",
        "    if x.shape[1] == 1:\n",
        "      x = x.squeeze(1)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "dtjary3bu-kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS     torch.nn.Sequential(*args: Module)"
      ],
      "metadata": {
        "id": "wTBQKVaIvSaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential:\n",
        "\n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  '''model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])  see model will be contained in such containers and called have parameters as list of layers that are processed below'''\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    # get parameters of all layers and stretch them out into one list\n",
        "    return [p for layer in self.layers for p in layer.parameters()]"
      ],
      "metadata": {
        "id": "kmSat8HyvRYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS    torch.optim.Optimizer(params, defaults)"
      ],
      "metadata": {
        "id": "PEyIREfh0et3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, parameters, lr=0.001):\n",
        "        self.parameters = parameters\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        for param in self.parameters:\n",
        "            param.data -= self.lr * param.grad.data\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.parameters:\n",
        "            if param.grad is not None:\n",
        "                param.grad.detach_()\n",
        "                param.grad.zero_()"
      ],
      "metadata": {
        "id": "KCcvOugW0UYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS    torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)"
      ],
      "metadata": {
        "id": "yGsIxRFT4GET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d:\n",
        "\n",
        "    # Weight initialization along with necessary variables\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.bias = bias\n",
        "\n",
        "        self.weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size)  # Random weight initialization\n",
        "        if self.bias:\n",
        "            self.bias = torch.zeros(out_channels)  # Zero bias initialization\n",
        "\n",
        "    def __call__(self, x):\n",
        "        batch_size, in_channels, in_height, in_width = x.size()\n",
        "        out_height = (in_height - self.kernel_size) // self.stride + 1\n",
        "        out_width = (in_width - self.kernel_size) // self.stride + 1\n",
        "\n",
        "        output = torch.zeros(batch_size, self.out_channels, out_height, out_width)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h_out in range(out_height):\n",
        "                    for w_out in range(out_width):\n",
        "                        h_start = h_out * self.stride\n",
        "                        w_start = w_out * self.stride\n",
        "                        h_end = h_start + self.kernel_size\n",
        "                        w_end = w_start + self.kernel_size\n",
        "\n",
        "                        receptive_field = x[b, :, h_start:h_end, w_start:w_end]\n",
        "                        output[b, c_out, h_out, w_out] = (receptive_field * self.weight[c_out]).sum() + self.bias[c_out] if self.bias else 0"
      ],
      "metadata": {
        "id": "X5vhFMqL22Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS   torch.nn.Softmax(dim=None)"
      ],
      "metadata": {
        "id": "WAZTn4Ky4JXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Softmax:\n",
        "    def __call__(self, x):\n",
        "        exp_vals = torch.exp(x)\n",
        "        sum_exp_vals = torch.sum(exp_vals, dim=1, keepdim=True)\n",
        "        return exp_vals / sum_exp_vals\n",
        "\n",
        "    def parameters(self):\n",
        "        return []"
      ],
      "metadata": {
        "id": "0BTrYR2k4nHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASS   torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0)"
      ],
      "metadata": {
        "id": "p6jM14Y-4oCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    def __call__(self, logits, targets):\n",
        "        logits_exp = torch.exp(logits)\n",
        "        logits_sum_exp = logits_exp.sum(dim=1, keepdim=True)\n",
        "        probabilities = logits_exp / logits_sum_exp\n",
        "\n",
        "        loss = -torch.log(probabilities[torch.arange(targets.size(0)), targets])\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "class MSELoss:\n",
        "    def __call__(self, predictions, targets):\n",
        "        loss = torch.mean((predictions - targets)**2)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "7VwZAmru_1iT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}